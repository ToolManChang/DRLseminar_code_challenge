{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "solving pendulum using actor-critic model\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Softmax\n",
    "from tensorflow.keras.layers import Add, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "from Blackjack import BlackjackEnv\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "def stack_samples(samples):\n",
    "\tarray = np.array(samples)\n",
    "\t\n",
    "\tcurrent_states = np.stack(array[:,0]).reshape((array.shape[0],-1))\n",
    "\tactions = np.stack(array[:,1]).reshape((array.shape[0],-1))\n",
    "\trewards = np.stack(array[:,2]).reshape((array.shape[0],-1))\n",
    "\tnew_states = np.stack(array[:,3]).reshape((array.shape[0],-1))\n",
    "\tdones = np.stack(array[:,4]).reshape((array.shape[0],-1))\n",
    "\t\n",
    "\treturn current_states, actions, rewards, new_states, dones\n",
    "\t\n",
    "\n",
    "# determines how to assign values to each state, i.e. takes the state\n",
    "# and action (two-input model) and determines the corresponding value\n",
    "class ActorCritic:\n",
    "\tdef __init__(self, env, sess):\n",
    "\t\tself.env  = env\n",
    "\t\tself.action_dim=2\n",
    "\t\tself.state_dim=104\n",
    "\t\tself.sess = sess\n",
    "        \n",
    "\n",
    "\t\tself.learning_rate = 0.0002\n",
    "\t\tself.epsilon = .99\n",
    "\t\tself.epsilon_decay = .99995\n",
    "\t\tself.gamma = .99\n",
    "\t\tself.tau   = .01\n",
    "\n",
    "\t\tself.alpha = tf.Variable([[1.]], trainable=True)\n",
    "\t\tself.target_entropy = -2.0\n",
    "\t\t# ===================================================================== #\n",
    "\t\t#                               Actor Model                             #\n",
    "\t\t# Chain rule: find the gradient of chaging the actor network params in  #\n",
    "\t\t# getting closest to the final value network predictions, i.e. de/dA    #\n",
    "\t\t# Calculate de/dA as = de/dC * dC/dA, where e is error, C critic, A act #\n",
    "\t\t# ===================================================================== #\n",
    "\t\tself.ema = tf.train.ExponentialMovingAverage(decay=1-self.tau)\n",
    "\t\tself.memory = deque(maxlen=40000)\n",
    "\t\tself.actor_state_input, self.actor_model = self.create_actor_model() \n",
    "\t\tself.logits = self.actor_model.output\n",
    "\t\t##action distribution\n",
    "\t\tself.action_dist = tf.nn.softmax(self.actor_model.output)\n",
    "\t\tself.action_dist = tf.expand_dims(self.action_dist, 1)\n",
    "\t\t##log distribution\n",
    "\t\tself.log_dist = tf.math.log(self.action_dist)\n",
    "\t\tself.log_dist = tf.reshape(self.log_dist,[-1,self.action_dim,1])\n",
    "\t\tprint(self.action_dist)  \n",
    "\t\tprint(self.log_dist)\n",
    "\n",
    "\t\t# ===================================================================== #\n",
    "\t\t#                              Critic Model                             #\n",
    "\t\t# ===================================================================== #\n",
    "\n",
    "\t\tself.action_input = tf.compat.v1.placeholder(tf.int32,shape = (None,1))\n",
    "\t\tself.critic_state_input, \\\n",
    "\t\t\tself.critic_model = self.create_critic_model()\n",
    "\t\tself.target_state_input, self.target_critic_model = self.create_critic_model()\n",
    "\n",
    "        #selected q value\n",
    "\t\tself.Q_value = tf.reshape(self.critic_model.output,[-1,2,1])\n",
    "\t\tself.one_hot = tf.expand_dims(tf.one_hot(self.action_input, self.action_dim),1)\n",
    "\t\tprint(self.Q_value)\n",
    "\t\tprint(self.one_hot)\n",
    "\t\tself.Q_value_sel = tf.squeeze(tf.linalg.matmul(self.one_hot,self.Q_value))\n",
    "\t\tprint(self.Q_value_sel)\n",
    "\t\tself.target_Q_value = tf.reshape(self.target_critic_model.output,[-1,2,1])\n",
    "\t\tself.value = tf.squeeze(tf.linalg.matmul(self.action_dist,self.Q_value-self.alpha*self.log_dist))\n",
    "\t\tself.target_value = tf.squeeze(tf.linalg.matmul(self.action_dist,self.target_Q_value-self.alpha*self.log_dist))\n",
    "\t\tprint(self.value)\n",
    "        \n",
    "\t\t# ===================================================================== #\n",
    "\t\t#                              loss funtion                             #\n",
    "\t\t# ===================================================================== #\n",
    "        \n",
    "\t\t##critic loss\n",
    "\t\tself.mask_target_value = tf.compat.v1.placeholder(tf.float32,shape=(None,1))\n",
    "\t\tself.critic_loss = tf.add_n([tf.compat.v1.losses.mean_squared_error(labels=self.mask_target_value, predictions=self.Q_value_sel, weights=0.5)])\n",
    "\t\tself.critic_opt = tf.compat.v1.train.AdamOptimizer(self.learning_rate).minimize(self.critic_loss)\n",
    "        \n",
    "\t\t##actor loss\n",
    "\t\tself.actor_weights = self.actor_model.trainable_weights\n",
    "\t\tself.actor_loss = tf.linalg.matmul(self.action_dist,self.alpha*self.log_dist-self.Q_value)\n",
    "\t\tself.actor_grad = tf.gradients(self.actor_loss,self.actor_weights)\n",
    "\t\tgrads = zip(self.actor_grad, self.actor_weights)\n",
    "\t\tself.actor_opt = tf.compat.v1.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)\n",
    "        \n",
    "\t\t##entropy loss\n",
    "\t\tself.entropy_loss = tf.linalg.matmul(self.action_dist,-self.alpha*(self.log_dist+self.target_entropy))\n",
    "\t\tprint(self.entropy_loss)\n",
    "\t\tself.entropy_grad = tf.gradients(self.entropy_loss,self.alpha)\n",
    "\t\tent_grads = zip(self.entropy_grad,itertools.repeat(self.alpha))\n",
    "\t\tself.entropy_opt = tf.compat.v1.train.AdamOptimizer(self.learning_rate).apply_gradients(ent_grads)\n",
    "\n",
    "                \n",
    "        \n",
    "\t\t# Initialize for later gradient calculations\n",
    "\t\tself.sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        \n",
    "\t# ========================================================================= #\n",
    "\t#                              Model Definitions                            #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef create_actor_model(self):\n",
    "\t\tstate_input = Input(shape=(self.state_dim,))\n",
    "\t\th1 = Dense(256, activation='relu')(state_input)\n",
    "\t\th2 = Dense(256, activation='relu')(h1)\n",
    "\t\tlogits = Dense(self.action_dim,activation='linear')(h2)\n",
    "\n",
    "\t\tmodel = Model([state_input], logits)\n",
    "\t\tadam  = Adam(lr=0.0001)\n",
    "\t\tmodel.compile(loss=\"mse\", optimizer=adam)\n",
    "\t\treturn state_input, model\n",
    "\n",
    "\tdef create_critic_model(self):\n",
    "\t\tstate_input = Input(shape=(self.state_dim,))\n",
    "\t\tstate_h1 = Dense(256, activation='relu')(state_input)\n",
    "\t\tstate_h2 = Dense(256, activation='relu')(state_h1)\n",
    "\n",
    "\t\toutput = Dense(self.action_dim, activation='linear')(state_h2)\n",
    "\t\tmodel  = Model([state_input],output)\n",
    "\n",
    "\t\tadam  = Adam(lr=0.0001)\n",
    "\t\tmodel.compile(loss=\"mse\", optimizer=adam)\n",
    "\t\treturn state_input, model\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                               Model Training                              #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef remember(self, cur_state, action, reward, new_state, done):\n",
    "\t\tself.memory.append([cur_state, action, reward, new_state, done])\n",
    "\n",
    "\tdef _train_actor(self, samples):\n",
    "\t\tcur_states, actions, rewards, new_states, _ =  stack_samples(samples)\n",
    "\n",
    "\t\tself.sess.run(self.actor_opt, feed_dict={\n",
    "            self.actor_state_input: cur_states,\n",
    "            self.critic_state_input:cur_states\n",
    "\t\t})\n",
    "\n",
    "\tdef _train_critic(self, samples):\n",
    "   \n",
    "\t\tcur_states, actions, rewards, new_states, dones = stack_samples(samples)\n",
    "\t\tfuture_rewards = self.sess.run(self.target_value,feed_dict={\n",
    "            self.actor_state_input:new_states,\n",
    "            self.target_state_input:new_states\n",
    "        })\n",
    "\t\trewards = rewards.reshape(future_rewards.shape) + self.gamma *np.multiply(future_rewards,(1 - dones.reshape(future_rewards.shape)))  \n",
    "\t\trewards = rewards.reshape((rewards.shape[0],1))\n",
    "\t\tself.sess.run(self.critic_opt, feed_dict={\n",
    "            self.mask_target_value:rewards,\n",
    "            self.action_input: actions,\n",
    "            self.critic_state_input:cur_states\n",
    "        })\n",
    "\t\t#print(evaluation.history)\n",
    "        \n",
    "\tdef _train_alpha(self, samples):\n",
    "\t\tcur_states, actions, rewards, new_states, _ =  stack_samples(samples)   \n",
    "\t\tself.sess.run(self.entropy_opt, feed_dict={\n",
    "\t\t\tself.actor_state_input: cur_states\n",
    "\t\t})\n",
    "        \n",
    "        \n",
    "\tdef train(self):\n",
    "\t\tbatch_size = 256\n",
    "\t\tif len(self.memory) < batch_size:\n",
    "\t\t\treturn\n",
    "\n",
    "\t\trewards = []\n",
    "\t\tsamples = random.sample(self.memory, batch_size)\n",
    "\t\tself.samples = samples\n",
    "\t\tself._train_critic(samples)\n",
    "\t\tself._train_alpha(samples)\n",
    "\t\tself._train_actor(samples)\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                         Target Model Updating                             #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "        \n",
    "\tdef _update_critic_target(self):\n",
    "\t\tcritic_model_weights  = self.critic_model.get_weights()\n",
    "\t\tcritic_target_weights = self.target_critic_model.get_weights()\n",
    "\t\t\n",
    "\t\tfor i in range(len(critic_target_weights)):\n",
    "\t\t\tcritic_target_weights[i] = critic_model_weights[i]*self.tau + critic_target_weights[i]*(1-self.tau)\n",
    "\t\tself.target_critic_model.set_weights(critic_target_weights)\n",
    "        \n",
    "\tdef update_target(self):\n",
    "\t\tself._update_critic_target()\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                              Model Predictions                            #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef act(self, cur_state):\n",
    "\t\tlogits = self.actor_model.predict(cur_state)\n",
    "\t\tsamples = tf.random.categorical(logits, cur_state.shape[0])\n",
    "\t\treturn self.sess.run(samples,feed_dict={self.actor_state_input: cur_state,})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_preprosser(cur_state):\n",
    "\tplayer_state = cur_state[0]\n",
    "\tdealer_state = np.zeros(52)\n",
    "\tdealer_state[cur_state[1]] = 1\n",
    "\treturn np.concatenate([player_state,dealer_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ExpandDims:0\", shape=(None, 1, 2), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(None, 2, 1), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(None, 2, 1), dtype=float32)\n",
      "Tensor(\"ExpandDims_1:0\", shape=(None, 1, 1, 2), dtype=float32)\n",
      "Tensor(\"Squeeze:0\", dtype=float32)\n",
      "Tensor(\"Squeeze_1:0\", dtype=float32)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Tensor(\"MatMul_4:0\", shape=(None, 1, 1), dtype=float32)\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "0.4\n",
      "0.8\n",
      "0.7\n",
      "0.7\n",
      "0.8\n",
      "0.5\n",
      "0.9\n",
      "0.5\n",
      "0.5\n",
      "0.8\n",
      "0.6\n",
      "0.8\n",
      "0.8\n",
      "0.6\n",
      "0.6\n",
      "0.5\n",
      "0.7\n",
      "0.7\n",
      "0.9\n",
      "0.6\n",
      "0.8\n",
      "0.9\n",
      "0.6\n",
      "0.6\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.7\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "1.0\n",
      "0.9\n",
      "0.7\n",
      "1.0\n",
      "0.7\n",
      "0.6\n",
      "0.8\n",
      "0.8\n",
      "0.7\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "1.0\n",
      "0.6\n",
      "1.0\n",
      "1.0\n",
      "0.8\n",
      "0.8\n",
      "1.0\n",
      "0.8\n",
      "1.0\n",
      "0.7\n",
      "1.0\n",
      "0.9\n",
      "0.9\n",
      "1.0\n",
      "0.9\n",
      "1.0\n",
      "0.9\n",
      "1.0\n",
      "1.0\n",
      "0.9\n",
      "0.9\n",
      "1.0\n",
      "0.9\n",
      "1.0\n",
      "0.9\n",
      "0.8\n",
      "1.0\n",
      "1.0\n",
      "0.9\n",
      "0.9\n",
      "0.8\n",
      "1.0\n",
      "0.9\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9\n",
      "0.9\n",
      "0.9\n",
      "0.9\n",
      "1.0\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "1.0\n",
      "0.9\n",
      "0.9\n",
      "0.8\n",
      "1.0\n",
      "1.0\n",
      "0.9\n",
      "0.9\n",
      "0.9\n",
      "0.9\n",
      "1.0\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9\n",
      "0.9\n",
      "0.9\n",
      "0.8\n",
      "1.0\n",
      "0.9\n",
      "0.9\n",
      "1.0\n",
      "0.9\n",
      "1.0\n",
      "0.7\n",
      "1.0\n",
      "0.7\n",
      "1.0\n",
      "1.0\n",
      "0.9\n",
      "1.0\n",
      "1.0\n",
      "0.8\n",
      "0.8\n",
      "1.0\n",
      "1.0\n",
      "0.8\n",
      "0.9\n",
      "0.9\n",
      "1.0\n",
      "1.0\n",
      "0.9\n",
      "0.8\n",
      "0.9\n",
      "0.9\n",
      "0.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3e9cdc7c6cf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-3e9cdc7c6cf8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m                                         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                                         \u001b[0mcur_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                                         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-807d8560c9b1>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, cur_state)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_state_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1337\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1341\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\tsess = tf.compat.v1.Session()\n",
    "\tK.set_session(sess)\n",
    "\tenv = BlackjackEnv({\"one_card_dealer\": True,\"card_values\":None})\n",
    "\tactor_critic = ActorCritic(env, sess)\n",
    "\n",
    "\tnum_trials = 100000\n",
    "\ttrial_len  = 10\n",
    "\n",
    "\tfor i in range(num_trials):\n",
    "\t\t#print(\"trial:\" + str(i))\n",
    "\t\tcur_state = env.reset()\n",
    "\t\taction = env.action_space.sample()\n",
    "\t\treward_sum = 0\n",
    "\t\tfor j in range(trial_len):\n",
    "\t\t\t#env.render()\n",
    "\t\t\tcur_state = state_preprosser(cur_state)\n",
    "\t\t\tcur_state = cur_state.reshape((1, cur_state.shape[0]))\n",
    "\t\t\taction = actor_critic.act(cur_state)\n",
    "\t\t\taction = action[0][0]\n",
    "\t\t\t#print(action)\n",
    "\t\t\tnew_state, reward, done, _ = env.step(action)\n",
    "\t\t\tif j == (trial_len - 1):\n",
    "\t\t\t\tdone = True\n",
    "\t\t\t\t#print(reward)\n",
    "\n",
    "\t\t\tif (j % 5 == 0):\n",
    "\t\t\t\tactor_critic.train()\n",
    "\t\t\t\tactor_critic.update_target()\n",
    "                \n",
    "\t\t\ttmp_state = cur_state                \n",
    "\t\t\tcur_state = new_state\n",
    "\t\t\tnew_state = state_preprosser(new_state)\n",
    "\t\t\t#new_state = new_state.reshape((1,new_state.shape[0]))\n",
    "\n",
    "\t\t\tactor_critic.remember(tmp_state, action, reward, new_state, done)\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\n",
    "\n",
    "\t\tif (i % 50 == 0):\n",
    "\n",
    "\t\t\ttrj_rewards = []\n",
    "\t\t\tfor traj in range(10): \n",
    "\t\t\t\tcur_state = env.reset()\n",
    "\t\t\t\trewards = 0\n",
    "\t\t\t\tfor j in range(10):\n",
    "\t\t\t\t\tcur_state = state_preprosser(cur_state)\n",
    "\t\t\t\t\tenv.render()\n",
    "\t\t\t\t\tcur_state = cur_state.reshape((1, cur_state.shape[0]))\n",
    "\t\t\t\t\taction = actor_critic.act(cur_state)\n",
    "\t\t\t\t\taction = action[0][0]\n",
    "\n",
    "\t\t\t\t\tnew_state, reward, done, _ = env.step(action)\n",
    "\t\t\t\t#reward += reward\n",
    "\t\t\t\t#if j == (trial_len - 1):\n",
    "\t\t\t\t\t#done = True\n",
    "\t\t\t\t\t#print(reward)\n",
    "\n",
    "\t\t\t\t#if (j % 5 == 0):\n",
    "\t\t\t\t#    actor_critic.train()\n",
    "\t\t\t\t#    actor_critic.update_target()   \n",
    "\t\t\t\t\trewards+=reward\n",
    "\t\t\t\t\tif done:\n",
    " \t\t\t\t\t\tbreak\n",
    "\n",
    "\n",
    "\t\t\t\t#actor_critic.remember(cur_state, action, reward, new_state, done)\n",
    "\t\t\t\t\tcur_state = new_state\n",
    "\t\t\t\ttrj_rewards.append(rewards)\n",
    "\t\t\tprint(np.mean(np.asarray(trj_rewards)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ExpandDims_6:0\", shape=(None, 1, 2), dtype=float32)\n",
      "Tensor(\"Reshape_9:0\", shape=(None, 2, 1), dtype=float32)\n",
      "Tensor(\"Reshape_10:0\", shape=(None, 2, 1), dtype=float32)\n",
      "Tensor(\"ExpandDims_7:0\", shape=(None, 1, 1, 2), dtype=float32)\n",
      "Tensor(\"Squeeze_9:0\", dtype=float32)\n",
      "Tensor(\"Squeeze_10:0\", dtype=float32)\n",
      "Tensor(\"MatMul_19:0\", shape=(None, 1, 1), dtype=float32)\n",
      "0.4\n",
      "0.6\n",
      "0.3\n",
      "0.3\n",
      "0.4\n",
      "0.4\n",
      "0.4\n",
      "0.2\n",
      "0.6\n",
      "0.7\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.4\n",
      "0.2\n",
      "0.4\n",
      "0.2\n",
      "0.3\n",
      "0.5\n",
      "0.2\n",
      "0.4\n",
      "0.4\n",
      "0.1\n",
      "0.2\n",
      "0.2\n",
      "0.6\n",
      "0.3\n",
      "0.2\n",
      "0.3\n",
      "0.0\n",
      "0.3\n",
      "0.4\n",
      "0.4\n",
      "0.4\n",
      "0.4\n",
      "0.2\n",
      "0.2\n",
      "0.5\n",
      "0.1\n",
      "0.4\n",
      "0.4\n",
      "0.6\n",
      "0.5\n",
      "0.3\n",
      "0.3\n",
      "0.1\n",
      "0.5\n",
      "0.8\n",
      "0.4\n",
      "0.2\n",
      "0.6\n",
      "0.3\n",
      "0.4\n",
      "0.4\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.4\n",
      "0.4\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.4\n",
      "0.4\n",
      "0.6\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.8\n",
      "0.5\n",
      "0.3\n",
      "0.5\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.4\n",
      "0.2\n",
      "0.2\n",
      "0.5\n",
      "0.5\n",
      "0.4\n",
      "0.6\n",
      "0.6\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.6\n",
      "0.3\n",
      "0.4\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.2\n",
      "0.3\n",
      "0.0\n",
      "0.7\n",
      "0.4\n",
      "0.7\n",
      "0.5\n",
      "0.4\n",
      "0.7\n",
      "0.2\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.4\n",
      "0.5\n",
      "0.5\n",
      "0.6\n",
      "0.3\n",
      "0.5\n",
      "0.7\n",
      "0.5\n",
      "0.2\n",
      "0.3\n",
      "0.7\n",
      "0.7\n",
      "0.5\n",
      "0.5\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.6\n",
      "0.3\n",
      "0.6\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.3\n",
      "0.5\n",
      "0.3\n",
      "0.6\n",
      "0.7\n",
      "0.5\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.4\n",
      "0.8\n",
      "0.5\n",
      "0.2\n",
      "0.0\n",
      "0.3\n",
      "0.4\n",
      "0.4\n",
      "0.5\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.2\n",
      "0.5\n",
      "0.3\n",
      "0.5\n",
      "0.3\n",
      "0.2\n",
      "0.4\n",
      "0.5\n",
      "0.3\n",
      "0.5\n",
      "0.9\n",
      "0.7\n",
      "0.5\n",
      "0.5\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.6\n",
      "0.1\n",
      "0.2\n",
      "0.2\n",
      "0.7\n",
      "0.5\n",
      "0.6\n",
      "0.3\n",
      "0.4\n",
      "0.3\n",
      "0.6\n",
      "0.4\n",
      "0.7\n",
      "0.5\n",
      "0.3\n",
      "0.7\n",
      "0.8\n",
      "0.1\n",
      "0.2\n",
      "0.9\n",
      "0.6\n",
      "0.6\n",
      "0.7\n",
      "0.4\n",
      "0.5\n",
      "0.2\n",
      "0.5\n",
      "0.6\n",
      "0.8\n",
      "0.6\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.6\n",
      "0.4\n",
      "0.5\n",
      "0.4\n",
      "0.4\n",
      "0.1\n",
      "0.6\n",
      "0.6\n",
      "0.4\n",
      "0.9\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.4\n",
      "0.5\n",
      "0.4\n",
      "0.8\n",
      "0.4\n",
      "0.4\n",
      "0.4\n",
      "0.5\n",
      "0.4\n",
      "0.9\n",
      "0.3\n",
      "0.5\n",
      "0.2\n",
      "0.3\n",
      "0.1\n",
      "0.5\n",
      "0.3\n",
      "0.5\n",
      "0.2\n",
      "0.6\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.2\n",
      "0.4\n",
      "0.5\n",
      "0.2\n",
      "0.4\n",
      "0.2\n",
      "0.5\n",
      "0.4\n",
      "0.4\n",
      "0.5\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.5\n",
      "0.5\n",
      "0.7\n",
      "0.3\n",
      "0.5\n",
      "0.4\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.5\n",
      "0.6\n",
      "0.5\n",
      "0.3\n",
      "0.6\n",
      "0.5\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.6\n",
      "0.5\n",
      "0.5\n",
      "0.3\n",
      "0.6\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "0.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d1b83d09bd5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mcur_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_preprosser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcur_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e0e433260ecd>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, cur_state)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_state_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1337\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1341\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "K.set_session(sess)\n",
    "env = BlackjackEnv()\n",
    "actor_critic = ActorCritic(env, sess)\n",
    "\n",
    "num_trials = 100000\n",
    "trial_len  = 10\n",
    "\n",
    "for i in range(num_trials):\n",
    "    #print(\"trial:\" + str(i))\n",
    "    cur_state = env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    reward_sum = 0\n",
    "    for j in range(trial_len):\n",
    "        #env.render()\n",
    "        cur_state = state_preprosser(cur_state)\n",
    "        cur_state = cur_state.reshape((1, cur_state.shape[0]))\n",
    "        action = actor_critic.act(cur_state)\n",
    "        action = action[0][0]\n",
    "        #print(action)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        if j == (trial_len - 1):\n",
    "            done = True\n",
    "            #print(reward)\n",
    "\n",
    "        if (j % 5 == 0):\n",
    "            actor_critic.train()\n",
    "            actor_critic.update_target()\n",
    "\n",
    "        tmp_state = cur_state                \n",
    "        cur_state = new_state\n",
    "        new_state = state_preprosser(new_state)\n",
    "        #new_state = new_state.reshape((1,new_state.shape[0]))\n",
    "\n",
    "        actor_critic.remember(tmp_state, action, reward, new_state, done)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    if (i % 50 == 0):\n",
    "\n",
    "        trj_rewards = []\n",
    "        for traj in range(10): \n",
    "            cur_state = env.reset()\n",
    "            rewards = 0\n",
    "            for j in range(10):\n",
    "                cur_state = state_preprosser(cur_state)\n",
    "                env.render()\n",
    "                cur_state = cur_state.reshape((1, cur_state.shape[0]))\n",
    "                action = actor_critic.act(cur_state)\n",
    "                action = action[0][0]\n",
    "\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "            #reward += reward\n",
    "            #if j == (trial_len - 1):\n",
    "                #done = True\n",
    "                #print(reward)\n",
    "\n",
    "            #if (j % 5 == 0):\n",
    "            #    actor_critic.train()\n",
    "            #    actor_critic.update_target()   \n",
    "                rewards+=reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "\n",
    "            #actor_critic.remember(cur_state, action, reward, new_state, done)\n",
    "                cur_state = new_state\n",
    "            trj_rewards.append(rewards)\n",
    "        print(np.mean(np.asarray(trj_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ExpandDims_2:0\", shape=(None, 1, 2), dtype=float32)\n",
      "Tensor(\"Reshape_3:0\", shape=(None, 2, 1), dtype=float32)\n",
      "Tensor(\"Reshape_4:0\", shape=(None, 2, 1), dtype=float32)\n",
      "Tensor(\"ExpandDims_3:0\", shape=(None, 1, 1, 2), dtype=float32)\n",
      "Tensor(\"Squeeze_3:0\", dtype=float32)\n",
      "Tensor(\"Squeeze_4:0\", dtype=float32)\n",
      "Tensor(\"MatMul_9:0\", shape=(None, 1, 1), dtype=float32)\n",
      "0.3\n",
      "0.5\n",
      "0.6\n",
      "0.5\n",
      "0.5\n",
      "0.6\n",
      "0.5\n",
      "0.7\n",
      "0.6\n",
      "0.8\n",
      "0.8\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.6\n",
      "0.3\n",
      "0.7\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "K.set_session(sess)\n",
    "env = BlackjackEnv({\"card_values\": np.ones(52,)*2})\n",
    "actor_critic = ActorCritic(env, sess)\n",
    "\n",
    "num_trials = 100000\n",
    "trial_len  = 10\n",
    "\n",
    "for i in range(num_trials):\n",
    "    #print(\"trial:\" + str(i))\n",
    "    cur_state = env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    reward_sum = 0\n",
    "    for j in range(trial_len):\n",
    "        #env.render()\n",
    "        cur_state = state_preprosser(cur_state)\n",
    "        cur_state = cur_state.reshape((1, cur_state.shape[0]))\n",
    "        action = actor_critic.act(cur_state)\n",
    "        action = action[0][0]\n",
    "        #print(action)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        if j == (trial_len - 1):\n",
    "            done = True\n",
    "            #print(reward)\n",
    "\n",
    "        if (j % 5 == 0):\n",
    "            actor_critic.train()\n",
    "            actor_critic.update_target()\n",
    "\n",
    "        tmp_state = cur_state                \n",
    "        cur_state = new_state\n",
    "        new_state = state_preprosser(new_state)\n",
    "        #new_state = new_state.reshape((1,new_state.shape[0]))\n",
    "\n",
    "        actor_critic.remember(tmp_state, action, reward, new_state, done)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    if (i % 50 == 0):\n",
    "\n",
    "        trj_rewards = []\n",
    "        for traj in range(10): \n",
    "            cur_state = env.reset()\n",
    "            rewards = 0\n",
    "            for j in range(10):\n",
    "                cur_state = state_preprosser(cur_state)\n",
    "                env.render()\n",
    "                cur_state = cur_state.reshape((1, cur_state.shape[0]))\n",
    "                action = actor_critic.act(cur_state)\n",
    "                action = action[0][0]\n",
    "\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "            #reward += reward\n",
    "            #if j == (trial_len - 1):\n",
    "                #done = True\n",
    "                #print(reward)\n",
    "\n",
    "            #if (j % 5 == 0):\n",
    "            #    actor_critic.train()\n",
    "            #    actor_critic.update_target()   \n",
    "                rewards+=reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "\n",
    "            #actor_critic.remember(cur_state, action, reward, new_state, done)\n",
    "                cur_state = new_state\n",
    "            trj_rewards.append(rewards)\n",
    "        print(np.mean(np.asarray(trj_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ExpandDims:0\", shape=(None, 1, 2), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(None, 2, 1), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(None, 2, 1), dtype=float32)\n",
      "Tensor(\"ExpandDims_1:0\", shape=(None, 1, 1, 2), dtype=float32)\n",
      "Tensor(\"Squeeze:0\", dtype=float32)\n",
      "Tensor(\"Squeeze_1:0\", dtype=float32)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Tensor(\"MatMul_4:0\", shape=(None, 1, 1), dtype=float32)\n",
      "0.7\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.9\n",
      "0.6\n",
      "0.6\n",
      "0.6\n",
      "0.6\n",
      "0.3\n",
      "0.3\n",
      "0.4\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.2\n",
      "0.3\n",
      "0.5\n",
      "0.3\n",
      "0.7\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.6\n",
      "0.3\n",
      "0.6\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.4\n",
      "0.4\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.4\n",
      "0.5\n",
      "0.3\n",
      "0.4\n",
      "0.4\n",
      "0.5\n",
      "0.4\n",
      "0.4\n",
      "0.6\n",
      "0.1\n",
      "0.3\n",
      "0.5\n",
      "0.4\n",
      "0.5\n",
      "0.4\n",
      "0.4\n",
      "0.3\n",
      "0.7\n",
      "0.4\n",
      "0.5\n",
      "0.4\n",
      "0.6\n",
      "0.6\n",
      "0.4\n",
      "0.5\n",
      "0.5\n",
      "0.4\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.3\n",
      "0.4\n",
      "0.4\n",
      "0.4\n",
      "0.5\n",
      "0.3\n",
      "0.7\n",
      "0.5\n",
      "0.4\n",
      "0.6\n",
      "0.7\n",
      "0.4\n",
      "0.7\n",
      "0.8\n",
      "0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-945a9104dff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcur_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_preprosser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcur_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#print(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-e0e433260ecd>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, cur_state)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_state_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1337\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1341\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "K.set_session(sess)\n",
    "env = BlackjackEnv({\"card_values\": [3,  1,  3,  9,  6,  0,  7, -2,  2,  6,  8,  1,  3,\n",
    "                                               4, -1,  4,  3,  9, -1,  4,  0,  4,  7, -2, -1,  5,\n",
    "                                               2,  6, -3, -1,  2,  2, -1,  7,  1,  0,  7,  8,  4,\n",
    "                                               5,  3, -1,  0,  3, -1,  3,  0,  6, -2,  4, -3,  4]})\n",
    "actor_critic = ActorCritic(env, sess)\n",
    "\n",
    "num_trials = 100000\n",
    "trial_len  = 10\n",
    "\n",
    "for i in range(num_trials):\n",
    "    #print(\"trial:\" + str(i))\n",
    "    cur_state = env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    reward_sum = 0\n",
    "    for j in range(trial_len):\n",
    "        #env.render()\n",
    "        cur_state = state_preprosser(cur_state)\n",
    "        cur_state = cur_state.reshape((1, cur_state.shape[0]))\n",
    "        action = actor_critic.act(cur_state)\n",
    "        action = action[0][0]\n",
    "        #print(action)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        if j == (trial_len - 1):\n",
    "            done = True\n",
    "            #print(reward)\n",
    "\n",
    "        if (j % 5 == 0):\n",
    "            actor_critic.train()\n",
    "            actor_critic.update_target()\n",
    "\n",
    "        tmp_state = cur_state                \n",
    "        cur_state = new_state\n",
    "        new_state = state_preprosser(new_state)\n",
    "        #new_state = new_state.reshape((1,new_state.shape[0]))\n",
    "\n",
    "        actor_critic.remember(tmp_state, action, reward, new_state, done)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    if (i % 50 == 0):\n",
    "\n",
    "        trj_rewards = []\n",
    "        for traj in range(10): \n",
    "            cur_state = env.reset()\n",
    "            rewards = 0\n",
    "            for j in range(10):\n",
    "                cur_state = state_preprosser(cur_state)\n",
    "                env.render()\n",
    "                cur_state = cur_state.reshape((1, cur_state.shape[0]))\n",
    "                action = actor_critic.act(cur_state)\n",
    "                action = action[0][0]\n",
    "\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "            #reward += reward\n",
    "            #if j == (trial_len - 1):\n",
    "                #done = True\n",
    "                #print(reward)\n",
    "\n",
    "            #if (j % 5 == 0):\n",
    "            #    actor_critic.train()\n",
    "            #    actor_critic.update_target()   \n",
    "                rewards+=reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "\n",
    "            #actor_critic.remember(cur_state, action, reward, new_state, done)\n",
    "                cur_state = new_state\n",
    "            trj_rewards.append(rewards)\n",
    "        print(np.mean(np.asarray(trj_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
